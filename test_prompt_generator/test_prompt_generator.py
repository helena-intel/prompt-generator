"""
Generating test prompts for transformer model testing. Prompts are generated by
cutting off input text (by default Alice In Wonderland) at the specified number of tokens.
"""

import json
import logging
import re
import warnings
from pathlib import Path

from transformers import AutoTokenizer

logging.basicConfig(level=logging.INFO)

# For most models, the tokenizer is the same for variations of the model
# This config allows specifying just "bert" instead of the full model_id, but
# there can always be model implementations that do things differently.
_preset_tokenizers = {
    "bert": "google-bert/bert-base-uncased",
    "bloom": "bigscience/bloom-560m",
    "gemma": "fxmarty/tiny-random-GemmaForCausalLM",
    "chatglm3": "THUDM/chatglm3-6b",
    "falcon": "tiiuae/falcon-7b",
    "gpt-neox": "EleutherAI/gpt-neox-20b",
    "llama": "TinyLlama/TinyLlama-1.1B-Chat-v0.6",
    "magicoder": "ise-uiuc/Magicoder-S-DS-6.7B",
    "mistral": "mistralai/Mistral-7B-v0.1",
    "opt": "facebook/opt-2.7b",
    "phi-2": "microsoft/phi-2",
    "pythia": "EleutherAI/pythia-1.4b-deduped",
    "roberta": "FacebookAI/roberta-base",
    "qwen": "Qwen/Qwen1.5-7B-Chat",
    "starcoder": "bigcode/starcoder2-7b",
    "t5": "google-t5/t5-base",
}


def generate_prompt(
    tokenizer_id: str,
    num_tokens: int,
    prefix: str = None,
    output_file: str = None,
    overwrite=False,
    verbose: bool = False,
    source_text_file: str = None,
    source_text: str = None,
):
    if source_text == "":
        source_text = None
    if source_text_file == "":
        source_text_file = None

    if source_text_file is not None and source_text is not None:
        raise ValueError("Only one of `source_text` or `source_text_file` should be provided.")
    if source_text_file is None and source_text is None:
        source_text_file = Path(__file__).parent / "text_files" / "alice.txt"

    if source_text is None:
        source_text = Path(source_text_file).read_text()
        if verbose:
            logging.info(f"Generating prompts from {source_text_file}")

    if tokenizer_id in _preset_tokenizers:
        tokenizer_id = _preset_tokenizers[tokenizer_id]

    tokenizer = AutoTokenizer.from_pretrained(tokenizer_id, trust_remote_code=True)

    num_tokens_from_source = num_tokens
    if tokenizer("hello")["input_ids"][-1] in tokenizer.all_special_ids:
        # The tokenizer adds a special token to the end of the sentence, so if
        # num_tokens = N, we need to get N-1 tokens from the input_text
        num_tokens_from_source -= 1

    if prefix is not None:
        prefix_tokens = tokenizer(prefix)["input_ids"]
        prefix_num_tokens = len(prefix_tokens)
        if prefix_num_tokens > num_tokens:
            logging.warning(
                f"Requested number of tokens {num_tokens} is smaller than "
                f"number of prefix tokens: {prefix_num_tokens}"
            )
        source_text = prefix + " " + source_text
    # Some tokenizers treat "\n\n" at the end of a sentence differently than in the middle;
    # replace consecutive "\n" with 1 to prevent generating an incorrect number of tokens
    source_text = re.sub(r"\n+", "\n", source_text)

    # prevent warnings about too many tokens from tokenizing the entire source text
    tokenizer.model_max_num_tokens = 10000
    with warnings.catch_warnings():
        warnings.filterwarnings("ignore", message=".*Token indices.*")

        inputs = tokenizer(source_text)

    tokens = inputs["input_ids"]
    total_tokens = len(tokens)
    prompt = None
    if num_tokens > total_tokens:
        raise ValueError(
            f"Cannot generate prompt with {num_tokens} tokens because the source text contains {total_tokens} tokens."
        )

    prompt = tokenizer.decode(tokens[:num_tokens_from_source], skip_special_tokens=True)
    if "chatglm" in tokenizer_id:
        # chatglm adds these tokens even when skip_special_tokens=True
        prompt = prompt.replace("[gMASK] sop ", "")

    if prompt is None:
        raise RuntimeError(f"Generating prompt with {num_tokens} tokens failed")

    prompt_tokens = tokenizer(prompt)
    if len(prompt_tokens["input_ids"]) != num_tokens:
        print("prompt", repr(prompt))
        print("prompt_tokens", prompt_tokens["input_ids"])
        print("source_tokens", tokens[:num_tokens])
        raise RuntimeError(
            f"Expected {num_tokens} tokens, got {len(prompt_tokens['input_ids'])}. Tokenizer: {tokenizer_id}"
        )

    if output_file is not None:
        if (Path(output_file).exists()) and (not overwrite):
            raise FileExistsError(f"{output_file} already exists. Set overwrite to allow overwriting.")
        prompt_dict = {
            "prompt": prompt,
            "model_id": tokenizer_id,
            "num_tokens": num_tokens,
        }
        Path(output_file).parent.mkdir(parents=True, exist_ok=True)
        with open(output_file, "w") as f:
            json.dump(prompt_dict, f, ensure_ascii=False, indent=2)

    return prompt


def main():
    import argparse

    parser = argparse.ArgumentParser()
    parser.add_argument(
        "-t",
        "--tokenizer",
        required=True,
        help="preset tokenizer id, model_id from Hugging Face hub, or path to local directory with tokenizer files. "
        f"Options for presets are: {list(_preset_tokenizers.keys())}",
    )
    parser.add_argument(
        "-n",
        "--num_tokens",
        required=True,
        type=int,
        help="Number of tokens the generated prompt should have",
    )
    parser.add_argument(
        "-p",
        "--prefix",
        required=False,
        help="Optional: prefix that the prompt should start with. Example: 'Translate to Dutch:'",
    )
    parser.add_argument(
        "-o",
        "--output_file",
        required=False,
        help="Optional: Path to store the prompt as .jsonl file",
    )
    parser.add_argument(
        "--overwrite",
        required=False,
        action="store_true",
        help="Overwrite output_file if it already exists.",
    )
    parser.add_argument("-v", "--verbose", action="store_true")
    parser.add_argument(
        "-f",
        "--file",
        required=False,
        help="Optional: path to text file to generate prompts from. Default text_files/alice.txt",
    )

    args = parser.parse_args()
    if args.verbose:
        logging.info(f"Command line arguments: {args}")

    return generate_prompt(
        tokenizer_id=args.tokenizer,
        num_tokens=args.num_tokens,
        prefix=args.prefix,
        output_file=args.output_file,
        overwrite=args.overwrite,
        verbose=args.verbose,
        source_text_file=args.file,
    )


if __name__ == "__main__":
    print(main())
